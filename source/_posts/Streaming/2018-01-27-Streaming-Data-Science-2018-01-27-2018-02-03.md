---
layout: post
title: "Streaming Data Science 2018.01.27 - 2018.02.03"
date: 2018-01-27
tags:
- kaggle
categories:
- StreamingDataScience
permalink: 
---

Log of Streaming.

<!-- more -->

## 2018-01-27 Saturday 
	
**Link:** [Kaggle: Porto Seguroâ€™s Safe Driver Prediction 1st place answer](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/44629#250927)

### Video

<iframe width="560" height="315" src="https://www.youtube.com/embed/7kE_xa9jc-s" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>

### Notes

* at least 6 models explored 
* final model: a mix of 6 models on same features
	* 1x <font color="red">lightgbm</font>
	* 5x nn

	procedure (loop 6 times)
	
	1. 6 models (i.e., 6 times)
		1. fit 1 models
		2. get the predicted value

		results: 6 predicted values from 6 mdoels respectively
	
	2. average 6 predicted values (all weights=1)

![](summary-of-6-models.png)

* `#1 + #2 + #3 + #4 + #5 + #6`: .2965
* `#1 + #2`: 0.29502 on private

<font color="blue">denoising autoencoder hidden activations</font>

Nonlinear stacking 

* Bootstrap Method: statistic varible of statistic variable

	Example: a sample of 100 values (x) 
	
	goal: get an estimate of the mean of the sample.

	* old way: calculate the mean directly from the sample: `mean(x) = 1/100 * sum(x)`

	* bootstrap:
	
	1. define sample size in each sub-samples,such as 1000
	2. define number of sub-samples, e.g. 3

		i.e., 3 sub-samples with 1000 samples each
	1. Create random sub-samples of dataset with replacement
	2. Calculate the mean of each sub-sample.
	2. Calculate the average of all of collected means
	3.  use that as estimated mean for the data.

Bootstrap Aggregation (Bagging): combine the predictions from multiple machine learning algorithms together to make more accurate predictions than any individual model.

* reduce the variance for those algorithm that have high variance. An algorithm that has high variance are decision trees, like **classification and regression trees (CART)**. <font color="blue">lower variance, increase bias</font>

	Example of CART: a sample dataset of 1000 instances (x) and we are using the CART algorithm. Bagging of the CART algorithm would work as follows.

	1. Create many (e.g. 100) random sub-samples of our dataset with replacement.
	1. Train a CART model on each sample.
	1. Given a new dataset, calculate the average prediction from each model.

### the way to share notes

I will post the notes on my blog.

1. easy search <= search bar
2. comment section for each note
3. embed YouTube video
4. achive page
